{"kind": "youtube#videoListResponse", "etag": "\"tnVOtk4NeGU6nDncDTE5m9SmuHc/-NN2N10R35CrGng8CrHUJknNtUA\"", "pageInfo": {"totalResults": 1, "resultsPerPage": 1}, "items": [{"kind": "youtube#video", "etag": "\"tnVOtk4NeGU6nDncDTE5m9SmuHc/gsT6s6l7odiDN-xktKbSCs879S8\"", "id": "R4OlXb9aTvQ", "snippet": {"publishedAt": "2013-11-27T13:23:11.000Z", "channelId": "UCotwjyJnb-4KW7bmsOoLfkg", "title": "Information Theory part 12: Information Entropy (Claude Shannon's formula)", "description": "Entropy is a measure of the uncertainty in a random variable (message source). Claude Shannon defines the \"bit\" as the unit of entropy (which is the uncertainty of a fair coin flip). In this video information entropy is introduced intuitively using bounce machines & yes/no questions. \n\nNote: This analogy applies to higher order approximations, we simply create a machine for each state and average over all machines!", "thumbnails": {"default": {"url": "https://i.ytimg.com/vi/R4OlXb9aTvQ/default.jpg", "width": 120, "height": 90}, "medium": {"url": "https://i.ytimg.com/vi/R4OlXb9aTvQ/mqdefault.jpg", "width": 320, "height": 180}, "high": {"url": "https://i.ytimg.com/vi/R4OlXb9aTvQ/hqdefault.jpg", "width": 480, "height": 360}, "standard": {"url": "https://i.ytimg.com/vi/R4OlXb9aTvQ/sddefault.jpg", "width": 640, "height": 480}, "maxres": {"url": "https://i.ytimg.com/vi/R4OlXb9aTvQ/maxresdefault.jpg", "width": 1280, "height": 720}}, "channelTitle": "Art of the Problem", "tags": ["entropy", "information", "information entropy", "bit", "information theory", "claude shannon", "measure", "language of coins", "art of the problem", "languageofcoins", "math"], "categoryId": "27", "liveBroadcastContent": "none", "localized": {"title": "Information Theory part 12: Information Entropy (Claude Shannon's formula)", "description": "Entropy is a measure of the uncertainty in a random variable (message source). Claude Shannon defines the \"bit\" as the unit of entropy (which is the uncertainty of a fair coin flip). In this video information entropy is introduced intuitively using bounce machines & yes/no questions. \n\nNote: This analogy applies to higher order approximations, we simply create a machine for each state and average over all machines!"}}}]}
