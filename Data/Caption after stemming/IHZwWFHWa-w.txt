last video laid structur neural network give quick recap fresh mind sand two main goal video first introduc idea gradient descent underli neural learn lot machin learn work well go dig littl particular network perform sand hidden neuron end actual look remind goal classic exampl hand written digit recognit hello world neural pix el grid pix el gray scale valu determin activ neuron input layer network activ neuron follow base weight sum activ previou layer plu special number cal led bia compos sum function like sigmoid ora rel u way last video total given somewhat arbitrari choic two hidden neuron network weight bia es adjust valu determin exactli network know actual mean say network given digit neuron final layer correspond digit rememb motiv mind layer structur mayb second layer could pick edg third layer might pick pattern like loop line sand last one could piec togeth pattern recogn learn network want algorithm show network whole bunch train data come form bunch differ imag hand written along suppos adjust weight bia es improv perform train data hope layer structur mean imag beyond train data way test train network show theta never seen see accur new us make common exampl start good peopl behind base put togeth collect thousand hand written digit imag one suppos provoc describ machin learn actual see lot less like crazi premis lot like well calculu exercis mean basic come find minimum certain function rememb conceptu think neuron connect neuron previou layer weight weight sum de fine activ kind like connect sand bia indic whether neuron tend activ inact start thing gon na initi weight bia es total randomli needless say network go perform pretti horribl given train exampl sinc someth random exampl feed imag output layer look like mess defin cost function way tell comput bad comput output activ zero neuron one neuron gave utter trash say littl mathemat add squar differ trash output activ valu want call cost singl train exampl notic sum small network confid imag correctli larg network seem like n realli know consid averag cost thousand train exampl dispos averag cost measur lousi network bad comput feel complic thing rememb network basic function one take input pix el valu spit ten output weight cost function layer complex top take input thirteen thousand weight bia es spit singl number de scribe bad weight bia es way defin depend network behavior thousand piec train data lot think tell comput job n help want tell chang weight bia es get better make easier rather struggl imagin function imagin simpl function one number input one number output find input minim valu function calculu student know sometim figur minimum explicitli alway feasibl realli complic thirteen thousand input version situat crazi complic neural network cost function flexibl tactic start old input figur direct step make output lower specif figur slope function shift left slope posit shift input right slope neg repeatedli point check ing new slope take appropri step gon na approach local minimum function imag might mind ball roll hill notic even realli simplifi singl input function mani possibl valley might land random input start guarante local minimum land go possibl valu cost function go carri neural network case well also want notic make step size proport slope slope flatten toward minimum step get smaller smaller kind help complex bit imagin instead function two input one output might think input space x plane cost function surfac instead slope function ask direct step input space decreas output function quickli word downhil direct help think ball roll hill familiar calculu know gradient function give direct ascent basic direct step increas function quickli natur enough take neg gradient give direct step decreas function quickli even length gradient vector actual indic steep slope unfamiliar calculu want learn check work khan academi topic honestli though matter right principl exist way comput vector vector tell downhil direct steep know rock solid get algorithm function comput gradient direct take small step downhil repeat basic idea function input instead two input imagin weight bia es network giant column vector neg gradient cost function vector direct insid insan huge input space tell go caus rapid decreas cost function cours special design cost weight bia es decreas mean make output network piec train data look less like random array ten valu like actual decis want make import rememb cost function involv averag train data minim mean better perform algorithm gradient effici effect heart neural network learn cal led back propag go talk next video realli want take time walk exactli happen weight bia given piec train data tri give intuit feel happen beyond pile relev calculu right main thing want know independ implement detail si mean talk network learn cost function notic one consequ import cost function nice smooth output find local minimum take littl step downhil way artifici neuron continu rang activ rather simpli activ inact binari way way biolog neuron process repeatedli input function multipl neg gradient cal led gradient descent way converg toward local minimum cost function basic valley still show pictur function two input cours nudg thirteen thousand dimension input space littl hard wrap mind around actual nice non spatial way think compon neg gradient tell us two thing sign cours tell us whether correspond compon input vector nudg importantli rel magnitud tell matter see network adjust one weight might much greater impact cost function adjust connect matter train data way think gradient vector cost function encod rel import weight bia go carri bang buck realli anoth way think direct take simpler exampl function two variabl input comput gradient particular point come one hand interpret say stand input move along direct increas function quickli graph function plane input point vector give straight uphil direct anoth way read say first variabl three time import second variabl least neighborhood relev x valu lot bang buck zoom sum far network function input output defin term weight cost function layer complex top take weight bia es input spit singl measur lousi ness base train exampl gradient cost function one layer complex still tell us nudg weight bia es caus chang valu cost function might interpret say weight matter initi network random weight bia es adjust mani time base gradient descent process well actual perform imag never seen well one describ two hidden sixteen neuron chosen mostli aesthet reason swell bad percent new imag correctli honestli look exampl kind feel compel led cut littl slack play around hidden layer structur make coupl get pretti good best certainli get better perform get sophist plain vanilla network given daunt initi task think someth incred network well imag never seen given never specif told pattern look origin way motiv structur de scribe hope might second layer might pick littl third layer would piec togeth edg recogn loop longer might piec togeth recogn network actual well one least rememb last video loo ked weight neuron first layer given neuron second layer visual given pix el pattern second layer neuron well actual weight associ transit first layer next instead isol littl edg look well almost random put loos pattern middl would seem unfathom dimension space possibl weight bia es network found happi littl local minimum despit success imag n exactli pick pattern might hope realli drive point home watch happen input random imag system smart might expect either feel uncertain mayb realli output neuron evenli instead confid give nonsens answer sure random nois actual imag differ even network recogn pretti well idea draw tightli constrain train setup mean put network point view entir univers consist noth clearli defin unmov center tini grid cost function never gave incent anyth utterli confid imag second layer neuron realli might wonder would introduc network motiv edg pattern si mean end well meant end goal instead start point frankli old technolog kind need understand understand detail modern variant clearli capabl interest dig hidden realli less intellig focu moment learn happen engag activ materi somehow one pretti simpl thing want paus right think deepli moment might make system perceiv imag wan ted better pick thing like edg pattern better actual engag recommend book deep learn neural network sin find code data load play exact exampl book walk step step code awesom book free publicli avail get someth consid join make donat toward effort si also link coupl resourc like lot descript phenomen beauti post la articl distil close thing last minut si want jump back snippet interview lei sha lee might rememb last video work deep learn littl snippet talk two recent realli dig modern imag recognit actual learn set convers first paper took one particularli deep neural realli good imag recognit instead train properli data set around train obvious test accuraci go better random sinc everyth randomli still abl achiev train accuraci would properli million weight particular network enough memor random data kind question whether cost function actual correspond sort structur imag know memor entir data set correct classif coupl know half year later exactli rebutt paper paper ad dress ked like hey actual someth littl bit smart er look accuraci curv train random data set curv sort went know slowli almost kind linear fashion realli struggl find local minima possibl know right weight would get accuraci wherea actual train structur data set one right know fiddl around littl bit begin kind drop ped fast get accuraci level sens easier find local maxima also interest caught bring light anoth paper actual coupl year ago lot network one result say look optim landscap local minima tend learn actual equal qualiti sens data set structur abl find much easili thank alway support said game changer video realli would possibl without also want give special thank v c firm partner support initi video seri focu earli stage machin learn ai feel pretti confid watch even like peopl know right earli get compani ground folk would love hear founder sand even set address video reach three blue one brown amplifi partner c om gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter gradient descent neural learn deep learn chapter