Last video I laid out the structure of a neural networkI 'll give a quick recap here just so that it 's fresh in our mindsAnd then I have two main goals for this video . The first is to introduce the idea of gradient descent , which underlies not only how neural networks learn , but how a lot of other machine learning works as wellThen after that we 're going to dig in a little more to how this particular network performsAnd what those hidden layers of neurons end up actually looking forAs a reminder our goal here is the classic example of handwritten digit recognitionthe hello world of neural networksthese digits are rendered on a 28 by 28 pixel grid each pixel with some grayscale value between 0 & 1those are what determine the activations of784 neurons in the input layer of the network andThen the activation for each neuron in the following layers is based on a weighted sum ofAll the activations in the previous layer plus some special number called a biasthen you compose that sum with some other function like the sigmoid squishification ora ReLu the way that I walked through last videoIn total given the somewhat arbitrary choice of two hidden layers here with 16 neurons each the network has about13,000 weights and biases that we can adjust and it 's these values that determine what exactly the network you know actually doesThen what we mean when we say that this network classifies a given digitIs that the brightest of those 10 neurons in the final layer corresponds to that digitAnd remember the motivation that we had in mind here for the layered structure was that maybeThe second layer could pick up on the edges and the third layer might pick up on patterns like loops and linesAnd the last one could just piece together those patterns to recognize digitsSo here we learn how the network learnsWhat we want is an algorithm where you can show this network a whole bunch of training datawhich comes in the form of a bunch of different images of handwritten digits along with labels for what they 're supposed to be andIt 'll adjust those13000 weights and biases so as to improve its performance on the training dataHopefully this layered structure will mean that what it learnsgeneralizes to images beyond that training dataAnd the way we test that is that after you train the networkYou show it more labeled theta that it 's never seen before and you see how accurately it classifies those new imagesFortunately for us and what makes this such a common example to start with is that the good people behind the MNIST base haveput together a collection of tens of thousands of handwritten digit images each one labeled with the numbers that they 're supposed to be andIt 's provocative as it is to describe a machine as learning once you actually see how it worksIt feels a lot less like some crazy sci-fi premise and a lot more like well a calculus exerciseI mean basically it comes down to finding the minimum of a certain functionRemember conceptually we 're thinking of each neuron as being connectedto all of the neurons in the previous layer and the weights in the weighted sum defining its activation are kind of like thestrengths of those connectionsAnd the bias is some indication of whether that neuron tends to be active or inactive and to start things offWe 're just gon na initialize all of those weights and biases totally randomly needless to say this network is going to performpretty horribly on a given training example since it 's just doing something random for example you feed in this image of a 3 and theOutput layer it just looks like a messSo what you do is you define a cost function a way of telling the computer : `` No bad computer ! That output should have activations which are zero for most neurons , but one for this neuron what you gave me is utter trash '' To say that a little more mathematically what you do is add up the squares of the differences betweeneach of those trash output activations and the value that you want them to have andThis is what we 'll call the cost of a single training exampleNotice this sum is small when the network confidently classifies the image correctlyBut it 's large when the network seems like it does n't really know what it 's doingSo then what you do is consider the average cost over all of the tens of thousands of training examples at your disposalThis average cost is our measure for how lousy the network is and how bad the computer should feel , and that 's a complicated thingRemember how the network itself was basically a function one that takes in784 numbers as inputs the pixel values and spits out ten numbers as its output and in a senseIt 's parameterised by all these weights and biasesWhile the cost function is a layer of complexity on top of that it takes as its inputthose thirteen thousand or so weights and biases and it spits out a single number describing how bad those weights and biases are andThe way it 's defined depends on the network 's behavior over all the tens of thousands of pieces of training dataThat 's a lot to think aboutBut just telling the computer what a crappy job , it 's doing is n't very helpfulYou want to tell it how to change those weights and biases so that it gets better ? To make it easier rather than struggling to imagine a function with 13,000 inputsJust imagine a simple function that has one number as an input and one number as an outputHow do you find an input that minimizes the value of this function ? Calculus students will know that you can sometimes figure out that minimum explicitlyBut that 's not always feasible for really complicated functionsCertainly not in the thirteen thousand input version of this situation for our crazy complicated neural network cost functionA more flexible tactic is to start at any old input and figure out which direction you should step to make that output lowerSpecifically if you can figure out the slope of the function where you areThen shift to the left if that slope is positive and shift the input to the right if that slope is negativeIf you do this repeatedly at each point checking the new slope and taking the appropriate stepyou 're gon na approach some local minimum of the function andthe image you might have in mind here is a ball rolling down a hill andNotice even for this really simplified single input function there are many possible valleys that you might land inDepending on which random input you start at and there 's no guarantee that the local minimumYou land in is going to be the smallest possible value of the cost functionThat 's going to carry over to our neural network case as well , and I also want you to noticeHow if you make your step sizes proportional to the slopeThen when the slope is flattening out towards the minimum your steps get smaller and smaller and that kind of helps you from overshootingBumping up the complexity a bit imagine instead a function with two inputs and one outputYou might think of the input space as the XY plane and the cost function as being graphed as a surface above itNow instead of asking about the slope of the function you have to ask which direction should you step in this input space ? So as to decrease the output of the function most quickly in other words . What 's the downhill direction ? And again it 's helpful to think of a ball rolling down that hillThose of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascentBasically , which direction should you step to increase the function most quicklynaturally enough taking the negative of that gradient gives you the direction to step that decreases the function most quickly andEven more than that the length of this gradient vector is actually an indication for just how steep that steepest slope isNow if you 're unfamiliar with multivariable calculusAnd you want to learn more check out some of the work that I did for Khan Academy on the topicHonestly , though all that matters for you and me right nowIs that in principle there exists a way to compute this vector . This vector that tells you what theDownhill direction is and how steep it is you 'll be okay if that 's all you know and you 're not rock solid on the detailsbecause if you can get that the algorithm from minimizing the function is to compute this gradient direction then take a small step downhill andJust repeat that over and overIt 's the same basic idea for a function that has 13,000 inputs instead of two inputs imagine organizing all13,000 weights and biases of our network into a giant column vectorThe negative gradient of the cost function is just a vectorIt 's some Direction inside this insanely huge input space that tells you whichnudges to all of those numbers is going to cause the most rapid decrease to the cost function andof course with our specially designed cost functionChanging the weights and biases to decrease it means making the output of the network on each piece of training dataLook less like a random array of ten values and more like an actual decision that we want it to makeIt 's important to remember this cost function involves an average over all of the training dataSo if you minimize it it means it 's a better performance on all of those samplesThe algorithm for computing this gradient efficiently which is effectively the heart of how a neural network learns is called back propagationAnd it 's what I 'm going to be talking about next videoThere I really want to take the time to walk throughWhat exactly happens to each weight and each bias for a given piece of training data ? Trying to give an intuitive feel for what 's happening beyond the pile of relevant calculus and formulasRight here right now the main thing . I want you to know independent of implementation detailsis that what we mean when we talk about a network learning is that it 's just minimizing a cost function andNotice one consequence of that is that it 's important for this cost function to have a nice smooth outputSo that we can find a local minimum by taking little steps downhillThis is why by the wayArtificial neurons have continuously ranging activations rather than simply being active or inactive in a binary wayif the way that biological neurons areThis process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descentIt 's a way to converge towards some local minimum of a cost function basically a valley in this graphI 'm still showing the picture of a function with two inputs of course because nudges in a thirteen thousand dimensional inputSpace are a little hard to wrap your mind around , but there is actually a nice non-spatial way to think about thisEach component of the negative gradient tells us two things the sign of course tells us whether the correspondingComponent of the input vector should be nudged up or down , but importantly the relative magnitudes of all these componentsKind of tells you which changes matter moreYou see in our network an adjustment to one of the weights might have a much greaterimpact on the cost function than the adjustment to some other weightSome of these connections just matter more for our training dataSo a way that you can think about this gradient vector of our mind-warpinglymassive cost function is that it encodes the relative importance of each weight and biasThat is which of these changes is going to carry the most bang for your buckThis really is just another way of thinking about directionTo take a simpler example if you have some function with two variables as an input and youCompute that its gradient at some particular point comes out as ( 3,1 ) Then on the one hand you can interpret that as saying that when you 're standing at that inputmoving along this direction increases the function most quicklyThat when you graph the function above the plane of input points that vector is what 's giving you the straight uphill directionBut another way to read that is to say that changes to this first variableHave three times the importance as changes to the second variable that at least in the neighborhood of the relevant inputNudging the x value carries a lot more bang for your buckAll rightLet 's zoom out and sum up where we are so far the network itself is this function with784 inputs and 10 outputs defined in terms of all of these weighted sumsthe cost function is a layer of complexity on top of that it takes the13,000 weights and biases as inputs and spits out a single measure of lousyness based on the training examples andThe gradient of the cost function is one more layer of complexity still it tells usWhat nudges to all of these weights and biases cause the fastest change to the value of the cost functionWhich you might interpret is saying which changes to which weights matter the mostSo when you initialize the network with random weights and biases and adjust them many times based on this gradient descent processHow well does it actually perform on images that it 's never seen before ? Well the one that I 've described here with the two hidden layers of sixteen neurons each chosen mostly for aesthetic reasonswell , it 's not bad it classifies about 96 percent of the new images that it sees correctly andHonestly , if you look at some of the examples that it messes up on you kind of feel compelled to cut it a little slackNow if you play around with the hidden layer structure and make a couple tweaksYou can get this up to 98 % and that 's pretty good . It 's not the bestYou can certainly get better performance by getting more sophisticated than this plain vanilla NetworkBut given how daunting the initial task is I just think there 's something ? Incredible about any network doing this well on images that it 's never seen beforeGiven that we never specifically told it what patterns to look forOriginally the way that I motivated this structure was by describing a hope that we might haveThat the second layer might pick up on little edgesThat the third layer would piece together those edges to recognize loops and longer lines and that those might be pieced together to recognize digitsSo is this what our network is actually doing ? Well for this one at leastNot at allremember how last video we looked at how the weights of theConnections from all of the neurons in the first layer to a given neuron in the second layerCan be visualized as a given pixel pattern that that second layer neuron is picking up onWell when we actually do that for the weights associated with these transitions from the first layer to the nextInstead of picking up on isolated little edges here and there . They look well almost randomJust put some very loose patterns in the middle there it would seem that in the unfathomably large13,000 dimensional space of possible weights and biases our network found itself a happy little local minimum thatdespite successfully classifying most images does n't exactly pick up on the patterns that we might have hoped for andTo really drive this point home watch what happens when you input a random imageif the system was smart you might expect it to either feel uncertain maybe not really activating any of those 10 output neurons orActivating them all evenlyBut instead itConfidently gives you some nonsense answer as if it feels as sure that this random noise is a 5 as it does that an actualimage of a 5 is a 5phrase differently even if this network can recognize digits pretty well it has no idea how to draw them aLot of this is because it 's such a tightly constrained training setupI mean put yourself in the network 's shoes here from its point of view the entire universe consists of nothingBut clearly defined unmoving digits centered in a tiny grid and its cost function just never gave it anyIncentive to be anything , but utterly confident in its decisionsSo if this is the image of what those second layer neurons are really doingYou might wonder why I would introduce this network with the motivation of picking up on edges and patternsI mean , that 's just not at all what it ends up doingWell , this is not meant to be our end goal , but instead a starting point franklyThis is old technologythe kind researched in the 80s and 90s andYou do need to understand it before you can understand more detailed modern variants and it clearly is capable of solving some interesting problemsBut the more you dig in to what those hidden layers are really doing the less intelligent it seemsShifting the focus for a moment from how networks learn to how you learnThat 'll only happen if you engage actively with the material here somehowOne pretty simple thing that I want you to do is just pause right now and think deeply for a moment about whatChanges you might make to this systemAnd how it perceives images if you wanted it to better pick up on things like edges and patterns ? But better than that to actually engage with the materialIHighly recommend the book by Michael Nielsen on deep learning and neural networksIn it you can find the code and the data to download and play with for this exact exampleAnd the book will walk you through step by step what that code is doingWhat 's awesome is that this book is free and publicly availableSo if you do get something out of it consider joining me in making a donation towards Nielsen 's effortsI 've also linked a couple other resources that I like a lot in the description including thephenomenal and beautiful blog post by Chris Ola and the articles in distillTo close things off here for the last few minutesI want to jump back into a snippet of the interview that I had with Leisha LeeYou might remember her from the last video . She did her PhD work in deep learning and in this little snippetShe talks about two recent papers that really dig into how some of the more modern image recognition networks are actually learningJust to set up where we were in the conversation the first paper took one of these particularly deep neural networksThat 's really good at image recognition and instead of training it on a properly labeled dataSet it shuffled all of the labels around before trainingObviously the testing accuracy here was going to be no better than random since everything 's just randomly labeledBut it was still able to achieve the same training accuracy as you would on a properly labeled datasetBasically the millions of weights for this particular network were enough for it to just memorize the random dataWhich kind of raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image ? Or is it just you know ? memorize the entireData set of what the correct classification is and so a couple of you know half a year later at ICML this yearThere was not exactly rebuttal paper paper that addressed some asked like heyActually these networks are doing something a little bit smarter than that if you look at that accuracy curveif you were just training on aRandom data set that curve sort of went down very you know very slowly in almost kind of a linear fashionSo you 're really struggling to find that local minima of possibleyou know the right weights that would get you that accuracy whereas if you 're actually training on a structured data set one that has theRight labels . You know you fiddle around a little bit in the beginning , but then you kind of dropped very fast to get to thatAccuracy level and so in some sense it was easier to find thatLocal maxima and so it was also interesting about that is it caught brings into light another paper from actually a couple of years agoWhich has a lot moresimplifications about the network layersBut one of the results was saying how if you look at the optimization landscape the local minima that these networks tend to learn areActually of equal quality so in some sense if your data set is structure , and you should be able to find that much more easilyMy thanks as always to those of you supporting on patreonI 've said before just what a game-changer patreon is but these videos really would not be possible without you IAlso want to give a special . Thanks to the VC firm amplifi partners in their support of these initial videos in the seriesThey focus on very early stage machine learning and AI companiesand I feel pretty confident in theProbabilities that some of you watching this and even more likely some of the people that you know areright now in the early stages of getting such a company off the ground andThe amplifi folks would love to hear from any such foundersand they even set up an email address just for this video that you can reach out to them through three blue one brown atamplify partners com test